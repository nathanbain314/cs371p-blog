---
layout: post
title: Week 14
---

###April 25 through 29###
###Past week ###
Since we didn’t have a project this past week I didn’t have as much work as I normally do so I spent more time working on projects for other classes. In class we spent much of the time learning about handle classes and how they should have been implemented in the Life project.  

###Challenges###
My challenge for this past week was that I needed to remember everything that we’ve gone over in class in order to study for the test. Without an actual project it was harder to force myself to pay as much attention to the class even though this next test is worth so much to my grade. 

###Next week###
Next week I will study for the test for a long while.

###Tip of the week###
There are 186 UTCS public computers. Linux defaults to some computer, formally sloth, but currently skipper, so never log into skipper or linux if you care about performance. Some are connected to the Condor computing cluster, others have multiple people logged in competing for memory and processing power. Last year when I took 439 one of the pintos tests would fail if it took too long, and it would take longer if I was testing on a machine with other users running heavy programs. Last semester in Programming for Performance I needed more reliable results when timing a program, and being on a machine with multiple people competing for resources would create much different results. The UTCS department has a [website](http://apps.cs.utexas.edu/unixlabstatus/) that shows the results of the “uptime” command on each machine. Using this you can choose a machine to log into has a lower load with fewer people if you need to.     
Last year when I was learning unix commands I came up with a janky solution to automate getting the computer with lowest load and fewest people. It can bre run as a single command.      
      
```
curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep "" -n | grep $(curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep "" -n | grep $(curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep "" -n | grep $(curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep "" -n | grep $(curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep '<td style="background-color: white; text-align:  left;">\|<td style="background-color: yellow; text-align:  left;">' -n | sed 's/[:].*$//' | awk '{ $1+=4} {print}' | xargs | sed -e 's/ /:\\|^/g' | sed -e 's/.*/^&:/g') | sed 's/[^>]*\(>.*\)/\1/' | sed 's/[<].*$//' | sed 's/^.//' | sort -n | sed -e '1!d')| sed 's/[:].*$//' | awk '{ $1-=1} {print}' | xargs | sed -e 's/ /:\\|^/g' | sed -e 's/.*/^&:/g') | grep $(curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep "" -n | grep $(curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep "" -n | grep $(curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep "" -n | grep $(curl -s http://apps.cs.utexas.edu/unixlabstatus/ | grep '<td style="background-color: white; text-align:  left;">\|<td style="background-color: yellow; text-align:  left;">' -n | sed 's/[:].*$//' | awk '{ $1+=4} {print}' | xargs | sed -e 's/ /:\\|^/g' | sed -e 's/.*/^&:/g') | sed 's/[^>]*\(>.*\)/\1/' | sed 's/[<].*$//' | sed 's/^.//' | sort -n | sed -e '1!d')| sed 's/[:].*$//' | awk '{ $1-=1} {print}' | xargs | sed -e 's/ /:\\|^/g' | sed -e 's/.*/^&:/g') | sed 's/[^>]*\(>.*\)/\1/' | sed 's/[<].*$//' | sed 's/^.//'| sort -n | sed -e '1d' | sed -e '1!d' | sed "s/.*/\>&\</") | sed -e '1!d' | sed 's/[:].*$//' | awk '{ $1-=3} {print}' | xargs | sed -e 's/ /:\\|^/g' | sed -e 's/.*/^&:/g')| sed 's/[^>]*\(>.*\)/\1/' | sed 's/[<].*$//' | sed 's/^.//'
```      
      
Basically it parses the site a couple of times and sorts the data, but I don’t really remember what any part actually does. It works almost all of the time but it doesn’t seem like a good solution. Calling ssh $(command) will ssh you into it from a UTCS computer. Calling ssh username@$(command).cs.utexas.edu will connect to it from any computer.     

